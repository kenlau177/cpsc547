abs(c(corPopLife,corPopGdp,corLifeGdp))
x = x %>% select(-country)
lifeExpTemp = x %>% filter(!is.na(lifeExp)) %>% select(lifeExp)
lifeExpTemp = lifeExpTemp[,1]
gdpPercapTemp = x %>% filter(!is.na(gdpPercap)) %>% select(gdpPercap)
gdpPercap = gdpPercapTemp[,1]
corLifeGdp = cor(lifeExpTemp, gdpPercapTemp)
corLifeGdp
outAmelia = amelia(x=x, m=m, p2s=0, idvars=c("batch","year","pop"),
empri=.01*nrow(trn),
logs=c(nameOfValue))
m=10
outAmelia = amelia(x=x, m=m, p2s=0, idvars=c("batch","year","pop"),
empri=.01*nrow(trn),
logs=c(nameOfValue))
outAmelia
computeMultiple = function(x, m=10, nameOfValue){
x = x %>% select(-country)
lifeExpTemp = x %>% filter(!is.na(lifeExp)) %>% select(lifeExp)
lifeExpTemp = lifeExpTemp[,1]
gdpPercapTemp = x %>% filter(!is.na(gdpPercap)) %>% select(gdpPercap)
gdpPercap = gdpPercapTemp[,1]
corLifeGdp = cor(lifeExpTemp, gdpPercapTemp)
if(abs(corLifeGdp)>.8){
return(list(computeKnn(x)))
} else {
outAmelia = amelia(x=x, m=m, p2s=0, idvars=c("batch","year","pop"),
empri=.01*nrow(trn),
logs=c(nameOfValue))
}
return(outAmelia$imputations)
}
trainLm = function(trn, test, fm){
fit = lm(formula=fm, data=trn)
pr = predict(fit, newdata=test)
return(pr)
}
computePredErrMult = function(x, nameOfValue, m=10){
# x = gtbl %>% filter(country == "Angola")
print(unique(x$country))
outSplitBatch = splitBatch(x)
trn = outSplitBatch$trn
test = outSplitBatch$test
fm = formula(paste0(nameOfValue, "~ year"))
multipleTrn = computeMultiple(trn, nameOfValue=nameOfValue)
multiplePr = llply(multipleTrn, .fun=trainLm, test=test, fm=fm)
multiplePr = unlist(multiplePr)
baggedPr = mean(multiplePr)
prErr = abs(test[,nameOfValue] - baggedPr)
names(prErr) = "pr"
return(prErr)
}
prErrMult = ddply(gtbl, .(country), .fun=computePredErrMult,
nameOfValue="lifeExp")
prErrMult
sum(prErrMult$pr^2)
continentData = gtbl %>% select(continent)
gtbl = gtbl %>% select(-continent)
continentData = gtbl %>% select(continent)
gtbl = gtbl %>% select(-continent)
computeKnn = function(x, k=5){
assert_that(is.data.frame(x))
if(has_name(x, "country")){
x = x %>% select(-country)
}
outSplitBatch = splitBatch(x)
trn = outSplitBatch$trn
test = outSplitBatch$test
#trn = trn %>% select(-batch)
#test = test %>% select(-batch)
trn = as.matrix(trn)
trn = impute.knn(trn, k=k)$data
trn = as.data.frame(trn)
out = rbind(trn, test)
return(out)
}
gtblByKnn = ddply(gtbl, .(country), .fun=computeKnn)
computePredErrKnn = function(x, nameOfValue){
outSplitBatch = splitBatch(x)
trn = outSplitBatch$trn
test = outSplitBatch$test
fm = formula(paste0(nameOfValue, "~ year"))
fit = lm(formula=fm, data=trn)
pr = predict(fit, newdata=test)
prErr = abs(test[,nameOfValue] - pr)
names(prErr) = "pr"
return(prErr)
}
prErrKnn = ddply(gtblByKnn, .(country), .fun=computePredErrKnn,
nameOfValue="lifeExp")
sum(prErrKnn$pr^2)
sum(prErrMult$pr^2)
citations()
citation()
L = do.call("rbind", list(L1, L2, L3, L4, L5, L6))
L1 = c(2, -1, -1, 0, 0, 0)
L2 = c(-1, 3, 0, -1, 0, -1)
L3 = c(-1, 0, 2, -1, 0, 0)
L4 = c(0, -1, -1, 3, -1, 0)
L5 = c(0, 0, 0, -1, 2, -1)
L6 = c(0, -1, 0, 0, -1, 2)
L = do.call("rbind", list(L1, L2, L3, L4, L5, L6))
L
eigen(L)
x = eigen(L)[,5]
x = sort(x)
x = eigen(L)$vectors[,5]
x = sort(x)
x
y = eigen(L)$vectors[,5]
x
u
data.frame(x,y)
mean(x)
x
install.packages("VGAM")
library(VGAM)   # zeta function
x <- (1-runif(10000))^(-1/(2.5-1))
plfit(x)
plfit
library(igraph)
plfit(x)
library(VGAM)   # zeta function
library(vgam)
plfit
g = read.graph("C://Users//Ken//Desktop//School//MSC2_Fall//Social_Network_Analysis//Assignments//assignment3//wikipedia.gml",format="gml")
summary(g)
g = read.graph("C://Users//Ken//Desktop//School//MSC2_Fall//Social_Network_Analysis//Assignments//assignment3//wikipedia.gml",format="gml")
summary(g)
degrees = degree(g,mode="all")
degrees
plfit<-function(x=rpareto(1000,10,2.5),method="limit",value=c(),finite=FALSE,nowarn=FALSE,nosmall=FALSE){
#init method value to NULL
vec <- c() ; sampl <- c() ; limit <- c(); fixed <- c()
###########################################################################################
#
#  test and trap for bad input
#
switch(method,
range = vec <- value,
sample = sampl <- value,
limit = limit <- value,
fixed = fixed <- value,
argok <- 0)
if(exists("argok")){stop("(plfit) Unrecognized method")}
if( !is.null(vec) && (!is.vector(vec) || min(vec)<=1 || length(vec)<=1) ){
print(paste("(plfit) Error: ''range'' argument must contain a vector > 1; using default."))
vec <- c()
}
if( !is.null(sampl) && ( !(sampl==floor(sampl)) ||  length(sampl)>1 || sampl<2 ) ){
print(paste("(plfit) Error: ''sample'' argument must be a positive integer > 2; using default."))
sample <- c()
}
if( !is.null(limit) && (length(limit)>1 || limit<1) ){
print(paste("(plfit) Error: ''limit'' argument must be a positive >=1; using default."))
limit <- c()
}
if( !is.null(fixed) && (length(fixed)>1 || fixed<=0) ){
print(paste("(plfit) Error: ''fixed'' argument must be a positive >0; using default."))
fixed <- c()
}
#  select method (discrete or continuous) for fitting and test if x is a vector
fdattype<-"unknow"
if( is.vector(x,"numeric") ){ fdattype<-"real" }
if( all(x==floor(x)) && is.vector(x) ){ fdattype<-"integer" }
if( all(x==floor(x)) && min(x) > 1000 && length(x) > 100 ){ fdattype <- "real" }
if( fdattype=="unknow" ){ stop("(plfit) Error: x must contain only reals or only integers.") }
#
#  end test and trap for bad input
#
###########################################################################################
###########################################################################################
#
#  estimate xmin and alpha in the continuous case
#
if( fdattype=="real" ){
xmins <- sort(unique(x))
xmins <- xmins[-length(xmins)]
if( !is.null(limit) ){
xmins <- xmins[xmins>=limit]
}
if( !is.null(fixed) ){
xmins <- fixed
}
if( !is.null(sampl) ){
xmins <- xmins[unique(round(seq(1,length(xmins),length.out=sampl)))]
}
dat <- rep(0,length(xmins))
z   <- sort(x)
for( xm in 1:length(xmins) ){
xmin <- xmins[xm]
z    <- z[z>=xmin]
n    <- length(z)
# estimate alpha using direct MLE
a    <- n/sum(log(z/xmin))
# truncate search if nosmall is selected
if( nosmall ){
if((a-1)/sqrt(n) > 0.1){
dat <- dat[1:(xm-1)]
print(paste("(plfit) Warning : xmin search truncated beyond",xmins[xm-1]))
break
}
}
# compute KS statistic
cx   <- c(0:(n-1))/n
cf   <- 1-(xmin/z)^a
dat[xm] <- max(abs(cf-cx))
}
D     <- min(dat)
xmin  <- xmins[min(which(dat<=D))]
z     <- x[x>=xmin]
n     <- length(z)
alpha <- 1 + n/sum(log(z/xmin))
if( finite ){
alpha <- alpha*(n-1)/n+1/n # finite-size correction
}
if( n<50 && !finite && !nowarn){
print("(plfit) Warning : finite-size bias may be present")
}
}
#
#  end continuous case
#
###########################################################################################
###########################################################################################
#
#  estimate xmin and alpha in the discrete case
#
if( fdattype=="integer" ){
if( is.null(vec) ){ vec<-seq(1.5,3.5,.01) } # covers range of most practical scaling parameters
zvec <- zeta(vec)
xmins <- sort(unique(x))
xmins <- xmins[-length(xmins)]
if( !is.null(limit) ){
limit <- round(limit)
xmins <- xmins[xmins>=limit]
}
if( !is.null(fixed) ){
xmins <- fixed
}
if( !is.null(sampl) ){
xmins <- xmins[unique(round(seq(1,length(xmins),length.out=sampl)))]
}
if( is.null(xmins) || length(xmins) < 2){
stop("(plfit) error: x must contain at least two unique values.")
}
if(length(which(xmins==0) > 0)){
stop("(plfit) error: x must not contain the value 0.")
}
xmax <- max(x)
dat <- matrix(0,nrow=length(xmins),ncol=2)
z <- x
for( xm in 1:length(xmins) ){
xmin <- xmins[xm]
z    <- z[z>=xmin]
n    <- length(z)
# estimate alpha via direct maximization of likelihood function
#  vectorized version of numerical calculation
# matlab: zdiff = sum( repmat((1:xmin-1)',1,length(vec)).^-repmat(vec,xmin-1,1) ,1);
if(xmin==1){
zdiff <- rep(0,length(vec))
}else{
zdiff <- apply(rep(t(1:(xmin-1)),length(vec))^-t(kronecker(t(array(1,xmin-1)),vec)),2,sum)
}
# matlab: L = -vec.*sum(log(z)) - n.*log(zvec - zdiff);
L <- -vec*sum(log(z)) - n*log(zvec - zdiff);
I <- which.max(L)
# compute KS statistic
fit <- cumsum((((xmin:xmax)^-vec[I])) / (zvec[I] - sum((1:(xmin-1))^-vec[I])))
cdi <- cumsum(hist(z,c(min(z)-1,(xmin+.5):xmax,max(z)+1),plot=FALSE)$counts/n)
dat[xm,] <- c(max(abs( fit - cdi )),vec[I])
}
D     <- min(dat[,1])
I     <- which.min(dat[,1])
xmin  <- xmins[I]
n     <- sum(x>=xmin)
alpha <- dat[I,2]
if( finite ){
alpha <- alpha*(n-1)/n+1/n # finite-size correction
}
if( n<50 && !finite && !nowarn){
print("(plfit) Warning : finite-size bias may be present")
}
}
#
#  end discrete case
#
###########################################################################################
#  return xmin, alpha and D in a list
return(list(xmin=xmin,alpha=alpha,D=D))
}
a = plfit(degrees)
a
cumy = c()
y = tabulate(degrees)
head(y)
?tabulate
length(y)
x = 1:length(y)
for (i in 1:length(x)) {
cumy[i] = sum(y[i:length(x)])/sum(y)
}
head(cumy)
options(scipen=10)
plot(x,cumy,log="xy",xlab="degree k",ylab="P(x) >= k",cex=0.5)
startval = cumy[a$xmin]
fittedvals = (a$xmin:max(x))^(-a$alpha + 1)*(startval)/a$xmin^(-a$alpha + 1)
points(a$xmin:max(x),fittedvals,type='l',col='red')
?degree
degreeDf = data.frame(degreeIn, "v"=V(g))
degreeIn = degree(g, mode="in")
degreeDf = data.frame(degreeIn, "v"=V(g))
head(degreeIn)
b = V(g)
head(b)
class(degreeIn)
class(b)
b = c(b)
class(b)
head(b)
?V
length(b)
length(degreeIn)
names(g)
attributes(g)
V(g)$label
names(V(g))
attributes(V(g))
str(V(g))
attr(V(g))
?V
attributes(V(g)$class)
attributes(V(g))$class
?igraph.vs
get.graph.attribute(g)
graph.attributes(g)
str(g)
names(str(g))
list.vertex.attribute(g)
list.vertex.attribute(V(g))
list.vertex.attributes(V(g))
list.vertex.attributes(g)
b = V(g)$label
head(b)
degreeDf = data.frame(degreeIn, "v"=V(g))
degreeDf = data.frame(degreeIn, "v"=V(g)$label)
dim(degreeDf)
head(degreeDf)
degreeDf = data.frame(degreeIn, "nodeName"=V(g)$label)
head(degreeDf)
library(dplyr)
?arrange
degreeDf = arrange(degreeDf, degreeIn)
head(degreeDf)
degreeDf = arrange(degreeDf, desc(degreeIn))
head(degreeDf)
degreeOut = degree(g, mode="out")
outDegreeDf = data.frame(degreeOut, "nodeName"=V(g)$label)
outDegreeDf = arrange(outDegreeDf, desc(degreeOut))
head(outDegreeDf)
degreeIn = degree(g, mode="in")
inDegreeDf = data.frame(degreeIn, "nodeName"=V(g)$label)
inDegreeDf = arrange(inDegreeDf, desc(degreeIn))
head(inDegreeDf)
outDegreeDf[1:100,]
V(g)$label[which.max(degreeOut)]
?betweeness
bb = betweeness(g, directed=F)
bb = betweenness(g, directed=F)
V(g)$label[V(g)[nei(which.max(bb))]]
?page.rank
pRank = page.rank(g)
pRank = pRank$vector
head(pRank)
length(pRank)
V(g)$label[which.max(pRank)]
head(inDegreeDf)
head(outDegreeDf)
?degree''
?degree
library(igraph)
install.packages("igraph")
g = read.graph("C://Users//Ken//Desktop//School//MSC2_Fall//Social_Network_Analysis//Assignments//Assignment4//wikipedia.gml",format="gml")
library(igraph)
g = read.graph("C://Users//Ken//Desktop//School//MSC2_Fall//Social_Network_Analysis//Assignments//Assignment4//wikipedia.gml",format="gml")
summary(g)
graph.coreness(as.undirected(g))
max(graph.coreness(as.undirected(g)))
gCliques = cliques((as.undirected(g)))
maxCliques = clique.number((as.undirected(g)))
maxCliques
fc = fastgreedy.community(as.undirected(g))
sizes(fc)
sum(size(fc)[1:4])
sum(sizes(fc)[1:4])
sum(sizes(fc))
sum(sizes(fc)[1:4])/sum(sizes(fc))
V(g)$label[membership(fc)==30]
imc = infomap.community(g)
names(imc)
size(imc)
sizes(imc)
head(imc)
head(sizes(imc))
head(sizes(fc))
?rank
?order
sizesOrdFc = sort(sizes(fc))
head(sizesOrdFc)
sizesOrdFc = sort(sizes(fc), decreasing=F)
sizesOrdFc
sizesOrdFc = sort(sizes(fc), decreasing=T)
head(sizesOrdFc)
sum(sizesOrdFc[1:4])/sum(sizesOrdFc)
sizesOrdImc = sort(sizes(imc), decreasing=T)
sizesOrdFc
head(sizesOrdImc)
head(sizesOrdFc)
histogram(sizesOrdFc)
hist(sizesOrdFc)
hist(sizesOrdImc)
modularity(fc)
membership(fc)
modularity(imc)
membership(fc)
modularity(fc)
modularity(imc)
library(devtools)
install.packages("devtools")
?install.packages
install.packages("devtools")
install.packages("devtools")
install.packages("devtool")
install.packages("devtools")
install.packages("devtools")
.libPaths()
library(devtools)
install_github("kenlau177/gameday")
library(gameday)
gday(team="Canucks", gDate="2014-11-19")
scores(gDate="2014-11-16")
todayDate = as.Date("2014-11-19")
endDate = as.Date("2014-12-19")
monthDates = strftime(seq(from=todayDate, to=endDate, by="day"))
# signify who your favourite team is
favouriteTeam = "Canucks"
# vector of booleans giving whether the team plays for each day
boolDates = unlist(lapply(monthDates, FUN=gday, team=favouriteTeam))
# returns a vector of dates of when your team will be playing in the next month
monthDates[boolDates]
library(plyr)
library(dplyr)
library(ggplot2)
todayDate = as.Date("2014-11-19")
# Note that this is one month back from today's date
endDate = as.Date("2014-10-19")
favouriteTeam = "Canucks"
boolDates = unlist(lapply(monthDates, FUN=gday, team=favouriteTeam))
# returns a vector of dates of when your team will be playing in the next month
scoreDays = monthDates[boolDates]
scoresDf = ldply(scoreDays, .fun=scores)
homeScores = scoresDf %>% select(home, home_score) %>%
filter(grepl(favouriteTeam, home, ignore.case=T)) %>%
select(home_score)
homeAvg = mean(homeScores[,1])
awayScores = scoresDf %>% select(away, away_score) %>%
filter(grepl(favouriteTeam, away, ignore.case=T)) %>%
select(away_score)
awayAvg = mean(awayScores[,1])
ggData = data.frame("location"=c("home","away"),
"average_score"=c(homeAvg, awayAvg))
gg = ggplot(ggData, aes(x=location, y=average_score)) +
geom_bar(stat="identity", width=.5) +
ggtitle("Average scores")
gg
sqrt(146)
sqrt(141)
length(-28:13)
length(-28:13)*50*6
length(-28:13)*60*6
length(-28:13)*200*6
runGitHub("kenlau177/cpsc547", subdir="/randomForestApp/")
library(shiny)
install.packages("shiny")
runGitHub("kenlau177/cpsc547", subdir="/randomForestApp/")
library(shiny)
runGitHub("kenlau177/cpsc547", subdir="/randomForestApp/")
setwd("C://Users//Ken//Documents//GitHub//cpsc547//randomForestApp")
library(shiny)
runApp()
urlTrain = "https://raw.githubusercontent.com/kenlau177/cpsc547/master/Data/datTrn_small.txt"
urlTrain = RCurl::getURL(urlTrain)
trainDat <- read.delim(file = urlTrain, sep=",")
rm(list=ls())
urlTrain = "Data//datTrn_small.txt"
trainDat <- read.delim(file = urlTrain, sep=",")
getwd()
urlTrain = "Data//datTrn_small.txt"
trainDat <- read.delim(file = urlTrain, sep=",")
runApp()
shinyapps::setAccountInfo(name=‘kenlau177’, token=‘2ECD09FABA5968508C79B31263E0DE40’, secret=‘yRb7YdA9LDyA1pc7xBZ4tdzT6YQeUwyRQR’)
shinyapps::setAccountInfo(name="kenlau177", token="2ECD09FABA5968508C79B31263E0DE40", secret="yRb7YdA9LDyA1pc7xBZ4tdzT6YQeUwyRQR")
library(shiny)
shinyapps::setAccountInfo(name="kenlau177", token="2ECD09FABA5968508C79B31263E0DE40", secret="yRb7YdA9LDyA1pc7xBZ4tdzT6YQeUwyRQR")
?shinyapps
install.pacakges("shiny")
install.packages("shiny")
install.packages("shiny")
library(shiny)
shinyapps::setAccountInfo(name="kenlau177", token="2ECD09FABA5968508C79B31263E0DE40", secret="yRb7YdA9LDyA1pc7xBZ4tdzT6YQeUwyRQR")
?shinyapps
shinyapps::setAccountInfo(name='kenlau177', token='2ECD09FABA5968508C79B31263E0DE40', secret='yRb7YdA9LDyA1pc7xBZ4tdzT6YQeUwyRQR/zwrNU')
